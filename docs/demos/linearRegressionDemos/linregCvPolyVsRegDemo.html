
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Ridge Regression with Polynomial Basis Expansion</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-02-25"><meta name="m-file" content="linregCvPolyVsRegDemo"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Ridge Regression with Polynomial Basis Expansion</h1><!--introduction--><p>Compare effect of regularizer strength</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Make the data</a></li><li><a href="#3">Basis function expansion</a></li><li><a href="#4">Now compare CV with train/test error on a dense grid of lambdas</a></li></ul></div><h2>Make the data<a name="1"></a></h2><pre class="codeinput">ns = [21 100];
<span class="keyword">for</span> ni=1:length(ns)
</pre><pre class="codeinput">   n = ns(ni);

<span class="comment">%[xtrain1d, ytrain, xtest1d, ytest] = polyDataMake('-sampling','thibaux','-n',n);</span>
xtrain = linspace(0,20,n)';
randn(<span class="string">'state'</span>, 654321);
xtest = [0:0.1:20]';
sigma2 = 4;
w = [-1.5; 1/9];
fun = @(x) w(1)*x + w(2)*x.^2;
ytrain = feval(fun, xtrain) + randn(size(xtrain,1),1)*sqrt(sigma2);
ytestNoisefree = feval(fun, xtest);
ytest = ytestNoisefree +  randn(size(xtest,1),1)*sqrt(sigma2);
</pre><h2>Basis function expansion<a name="3"></a></h2><pre class="codeinput">deg = 14;
[Xtrain] = rescaleData(xtrain);
Xtrain = degexpand(Xtrain, deg, false);
[Xtest] = rescaleData(xtest);
Xtest = degexpand(Xtest, deg, false);
</pre><h2>Now compare CV with train/test error on a dense grid of lambdas<a name="4"></a></h2><pre class="codeinput">lambdas = logspace(-10,1.2,9);
<span class="comment">%lambdas = logspace(-5,1.2,9);</span>
NL = length(lambdas);
logev = zeros(1,NL); testMse = zeros(1,NL); trainMse = zeros(1,NL);
<span class="keyword">for</span> k=1:NL
    lambda = lambdas(k);
    fitFn = @(Xtr,ytr) linregFitL2(Xtr, ytr, lambda);
    predFn = @(w, Xte) linregPredict(w, Xte);
    lossFn = @(yhat, yte)  (yhat - yte).^2;
    [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, 5);

    w = linregFitL2(Xtrain, ytrain, lambda);
    ypredTest = linregPredict(w, Xtest);
    ypredTrain = linregPredict(w, Xtrain);
    testMse(k) = mean((ypredTest - ytest).^2);
    trainMse(k) = mean((ypredTrain - ytrain).^2);
<span class="keyword">end</span>


figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
plot(ndx, trainMse, <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
plot(ndx, testMse, <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
xlabel(<span class="string">'log regularizer'</span>)
ylabel(<span class="string">'mse'</span>)
<span class="comment">%plot(ndx, mu, 'ko-', 'linewidth', 2, 'markersize', 12);</span>
errorbar(ndx, mu, se, <span class="string">'ko-'</span>,<span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12 );
legend(<span class="string">'train'</span>, <span class="string">'test'</span>, <span class="string">'5-CV'</span>)
title(sprintf(<span class="string">'ridge regression, ntrain = %d'</span>, n))
<span class="keyword">if</span> n==21
   set(gca,<span class="string">'yscale'</span>,<span class="string">'log'</span>)
   <span class="comment">%set(gca,'ylim',[0 1000]);</span>
   ylabel(<span class="string">'log mse'</span>)
<span class="keyword">end</span>


<span class="comment">% draw vertical line at best value</span>
dof = 1./(eps+lambdas);
idx_opt = oneStdErrorRule(mu, se, dof);
ylim = get(gca, <span class="string">'ylim'</span>);
x = ndx(idx_opt);
h=line([x x], ylim);
set(h, <span class="string">'color'</span>, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 2);

printPmtkFigure(sprintf(<span class="string">'linregCVPolyVsReg%d-mse'</span>, n))
</pre><img vspace="5" hspace="5" src="linregCvPolyVsRegDemo_01.png" alt=""> <img vspace="5" hspace="5" src="linregCvPolyVsRegDemo_02.png" alt=""> <pre class="codeinput"><span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Ridge Regression with Polynomial Basis Expansion
% Compare effect of regularizer strength

%% Make the data

ns = [21 100];
for ni=1:length(ns)
   n = ns(ni);

%[xtrain1d, ytrain, xtest1d, ytest] = polyDataMake('-sampling','thibaux','-n',n);
xtrain = linspace(0,20,n)';
randn('state', 654321);
xtest = [0:0.1:20]';
sigma2 = 4;
w = [-1.5; 1/9];
fun = @(x) w(1)*x + w(2)*x.^2;
ytrain = feval(fun, xtrain) + randn(size(xtrain,1),1)*sqrt(sigma2);
ytestNoisefree = feval(fun, xtest);
ytest = ytestNoisefree +  randn(size(xtest,1),1)*sqrt(sigma2);

%% Basis function expansion
deg = 14;
[Xtrain] = rescaleData(xtrain);
Xtrain = degexpand(Xtrain, deg, false);
[Xtest] = rescaleData(xtest);
Xtest = degexpand(Xtest, deg, false);


%% Now compare CV with train/test error on a dense grid of lambdas
lambdas = logspace(-10,1.2,9);
%lambdas = logspace(-5,1.2,9);
NL = length(lambdas);
logev = zeros(1,NL); testMse = zeros(1,NL); trainMse = zeros(1,NL);
for k=1:NL
    lambda = lambdas(k);
    fitFn = @(Xtr,ytr) linregFitL2(Xtr, ytr, lambda);
    predFn = @(w, Xte) linregPredict(w, Xte);
    lossFn = @(yhat, yte)  (yhat - yte).^2;
    [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, 5);
    
    w = linregFitL2(Xtrain, ytrain, lambda);
    ypredTest = linregPredict(w, Xtest);
    ypredTrain = linregPredict(w, Xtrain);
    testMse(k) = mean((ypredTest - ytest).^2); 
    trainMse(k) = mean((ypredTrain - ytrain).^2);
end


figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
plot(ndx, trainMse, 'bs:', 'linewidth', 2, 'markersize', 12);
plot(ndx, testMse, 'rx-', 'linewidth', 2, 'markersize', 12);
xlabel('log regularizer')
ylabel('mse')
%plot(ndx, mu, 'ko-', 'linewidth', 2, 'markersize', 12);
errorbar(ndx, mu, se, 'ko-','linewidth', 2, 'markersize', 12 );
legend('train', 'test', '5-CV')
title(sprintf('ridge regression, ntrain = %d', n))
if n==21
   set(gca,'yscale','log')
   %set(gca,'ylim',[0 1000]);
   ylabel('log mse')
end


% draw vertical line at best value
dof = 1./(eps+lambdas);
idx_opt = oneStdErrorRule(mu, se, dof);
ylim = get(gca, 'ylim');
x = ndx(idx_opt);
h=line([x x], ylim);
set(h, 'color', 'k', 'linewidth', 2);

printPmtkFigure(sprintf('linregCVPolyVsReg%d-mse', n))


end

##### SOURCE END #####
--></body></html>