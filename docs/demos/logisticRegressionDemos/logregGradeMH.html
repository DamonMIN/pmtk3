
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>logregGradeMH</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-02-25"><meta name="m-file" content="logregGradeMH"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><pre class="codeinput"><span class="keyword">function</span> [X, y, ws, perm] = logregGradeMH()
<span class="comment">% Example from Johnson and Albert p87</span>

[X,y] = satDataLoad;
model = logregBinaryFitL2IRLS(X, y, 1e-8, false);
w = model.w;
C = model.C;


<span class="comment">% MH</span>
setSeed(1);
xinit = w;
Nsamples = 5000;
lambda = 0;
targetArgs = {X,y,lambda};
sigmaMH = 1.5;
proposalArgs = {sigmaMH*C};

[ws, naccept] = metropHastings(@logpost, @proposal, xinit, Nsamples, targetArgs, proposalArgs);

<span class="comment">% trace plots</span>
figure
<span class="keyword">for</span> i=1:2
    subplot(2,2,i)
    plot(ws(:,i))
<span class="keyword">end</span>

<span class="comment">% samples</span>
figure
subplot(2,2,1)
hist(ws(:,2))
title(<span class="string">'w1 slope'</span>)
subplot(2,2,2)
plot(ws(:,1), ws(:,2), <span class="string">'.'</span>)
xlabel(<span class="string">'w0'</span>); ylabel(<span class="string">'w1'</span>)
subplot(2,2,4)
hist(ws(:,1))
title(<span class="string">'w0 intercept'</span>)

MLE =  xinit
postMean  = mean(ws,1)
postMedian = median(ws,1)

<span class="comment">% visualize model fit for each training point</span>
figure
perm = sortidx(X(:, 2), <span class="string">'ascend'</span>);
N = length(perm);
<span class="keyword">for</span> ii=1:N
    i = perm(ii);
    ps = 1 ./ (1+exp(-X(i,:)*ws')); <span class="comment">% ps(s) = p(y=1|x(i,:), bs(s,:)) row vec</span>

    plot(X(i,2), median(ps), <span class="string">'o'</span>);
    hold <span class="string">on</span>
    h=plot(X(i,2), y(i), <span class="string">'ko'</span>);
    set(h,<span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);

    <span class="comment">% prediction interval</span>
    tmp = sort(ps, <span class="string">'ascend'</span>);
    Q5 = tmp(floor(0.05*Nsamples));
    Q95 = tmp(floor(0.95*Nsamples));
    line([X(i,2) X(i,2)], [Q5 Q95]);
<span class="keyword">end</span>

logregSATdemo

<span class="keyword">end</span>

<span class="comment">%%%%%%%%%</span>
<span class="keyword">function</span> bnew = proposal(w, Sigma)
    model.mu = zeros(1, length(w));
    model.Sigma = Sigma;
    bnew = w + gaussSample(model);
<span class="keyword">end</span>

<span class="keyword">function</span> p = logpost(w, X, y, lambda)
    mu = 1 ./ (1 + exp(-X*w(:)));
    p = sum( (y.*log(mu) + (1-y).*log(1-mu))) + lambda/2*sum(w.^2);
<span class="keyword">end</span>









<span class="comment">%{
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">stat = load('satData.txt'); % Johnson and Albert p77 table 3.1
</span><span class="comment">% stat=[pass(0/1), 1, 1, sat_score, grade in prereq]
</span><span class="comment">% where the grade in prereq is encoded as A=5,B=4,C=3,D=2,F=1
</span><span class="comment">y = stat(:,1);
</span><span class="comment">N = length(y);
</span><span class="comment">X = stat(:,4);
</span><span class="comment">X1 = [ones(N,1) X];
</span><span class="comment">
</span><span class="comment">lambda = 1e-10;
</span><span class="comment">model = logregBinaryFitL2IRLS(X,y, lambda);
</span><span class="comment">beta = model.w;
</span><span class="comment">C = model.C;
</span><span class="comment">
</span><span class="comment">% MH
</span><span class="comment">setSeed(1);
</span><span class="comment">xinit = beta;
</span><span class="comment">Nsamples = 10000;
</span><span class="comment">lambda = 0;
</span><span class="comment">sigmaMH = 1.5;
</span><span class="comment">%targetArgs = {X,y,lambda};
</span><span class="comment">%proposalArgs = {sigmaMH*C};
</span><span class="comment">target = @(b) logpost(b, X1, y, lambda);
</span><span class="comment">prop = @(b) proposal(b, sigmaMH*C);
</span><span class="comment">[bs, acceptRatio] = metropolisHastings(target, prop, xinit, Nsamples);
</span><span class="comment">%[bs, naccept] = metrop(@logpost, @proposal, xinit, Nsamples,  targetArgs, proposalArgs);
</span><span class="comment">
</span><span class="comment">% trace plots
</span><span class="comment">figure
</span><span class="comment">for i=1:2
</span><span class="comment">  subplot(2,2,i)
</span><span class="comment">  plot(bs(:,i))
</span><span class="comment">end
</span><span class="comment">
</span><span class="comment">% samples
</span><span class="comment">figure
</span><span class="comment">subplot(2,2,1)
</span><span class="comment">hist(bs(:,2))
</span><span class="comment">title('b1 slope')
</span><span class="comment">subplot(2,2,2)
</span><span class="comment">plot(bs(:,1), bs(:,2), '.')
</span><span class="comment">xlabel('b0'); ylabel('b1')
</span><span class="comment">subplot(2,2,4)
</span><span class="comment">hist(bs(:,1))
</span><span class="comment">title('b0 intercept')
</span><span class="comment">
</span><span class="comment">MLE =  xinit
</span><span class="comment">postMean  = mean(bs,1)
</span><span class="comment">postMedian = median(bs,1)
</span><span class="comment">
</span><span class="comment">% visualize model fit for each training point
</span><span class="comment">figure
</span><span class="comment">[junk,perm] = sort(X,'ascend');
</span><span class="comment">N = length(perm);
</span><span class="comment">for ii=1:N
</span><span class="comment">  i = perm(ii);
</span><span class="comment">  ps = 1 ./ (1+exp(-X1(i,:)*bs')); % ps(s) = p(y=1|x(i,:), bs(s,:)) row vec
</span><span class="comment">
</span><span class="comment">  plot(X(i,1), median(ps), 'o');
</span><span class="comment">  hold on
</span><span class="comment">  h=plot(X(i,1), y(i), 'ko');
</span><span class="comment">  set(h,'markerfacecolor', 'k');
</span><span class="comment">
</span><span class="comment">  % prediction interval
</span><span class="comment">  tmp = sort(ps, 'ascend');
</span><span class="comment">  Q5 = tmp(floor(0.05*Nsamples));
</span><span class="comment">  Q95 = tmp(floor(0.95*Nsamples));
</span><span class="comment">  line([X(i) X(i)], [Q5 Q95]);
</span><span class="comment">end
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">%%%%%%%%%
</span><span class="comment">function bnew = proposal(b, Sigma)
</span><span class="comment">    model = struct('mu', zeros(1, length(b)), 'Sigma', Sigma);
</span><span class="comment">    bnew = b + gaussSample(model);
</span><span class="comment">
</span><span class="comment">function p = logpost(b, X, y, lambda)
</span><span class="comment">logprior = 0;  % log(1)
</span><span class="comment">offsetAdded = true;
</span><span class="comment">fn = @(w)LogisticLossSimple(w, X, y);
</span><span class="comment">p = -penalizedL2(b(:), fn, lambda) + logprior;
</span><span class="comment">%p = -logregL2NLLgradHess(b(:), X, y, lambda, offsetAdded) + logprior;
</span><span class="comment">
</span><span class="comment">%}</span>
</pre><pre class="codeoutput">MLE =
  -31.1146
    0.0578
postMean =
  -37.7267    0.0699
postMedian =
  -36.1595    0.0671
Warning: Log of zero. This warning will be
removed in a future release.
         Consider using DBSTOP IF NANINF when
         debugging. 
Warning: Log of zero. This warning will be
removed in a future release.
         Consider using DBSTOP IF NANINF when
         debugging. 
Warning: Log of zero. This warning will be
removed in a future release.
         Consider using DBSTOP IF NANINF when
         debugging. 
Warning: Log of zero. This warning will be
removed in a future release.
         Consider using DBSTOP IF NANINF when
         debugging. 
Warning: Log of zero. This warning will be
removed in a future release.
         Consider using DBSTOP IF NANINF when
         debugging. 
ans =
   463
   471
   488
   517
   525
   525
   533
   543
   545
   549
   553
   557
   557
   559
   563
   572
   574
   574
   576
   576
   581
   582
   582
   584
   584
   591
   595
   599
   609
   649
</pre><img vspace="5" hspace="5" src="logregGradeMH_01.png" alt=""> <img vspace="5" hspace="5" src="logregGradeMH_02.png" alt=""> <img vspace="5" hspace="5" src="logregGradeMH_03.png" alt=""> <img vspace="5" hspace="5" src="logregGradeMH_04.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
function [X, y, ws, perm] = logregGradeMH()
% Example from Johnson and Albert p87

[X,y] = satDataLoad;
model = logregBinaryFitL2IRLS(X, y, 1e-8, false);
w = model.w;
C = model.C;


% MH
setSeed(1);
xinit = w;
Nsamples = 5000;
lambda = 0;
targetArgs = {X,y,lambda};
sigmaMH = 1.5;
proposalArgs = {sigmaMH*C};

[ws, naccept] = metropHastings(@logpost, @proposal, xinit, Nsamples, targetArgs, proposalArgs);

% trace plots
figure
for i=1:2
    subplot(2,2,i)
    plot(ws(:,i))
end

% samples
figure
subplot(2,2,1)
hist(ws(:,2))
title('w1 slope')
subplot(2,2,2)
plot(ws(:,1), ws(:,2), '.')
xlabel('w0'); ylabel('w1')
subplot(2,2,4)
hist(ws(:,1))
title('w0 intercept')

MLE =  xinit
postMean  = mean(ws,1)
postMedian = median(ws,1)

% visualize model fit for each training point
figure
perm = sortidx(X(:, 2), 'ascend');
N = length(perm);
for ii=1:N
    i = perm(ii);
    ps = 1 ./ (1+exp(-X(i,:)*ws')); % ps(s) = p(y=1|x(i,:), bs(s,:)) row vec
    
    plot(X(i,2), median(ps), 'o');
    hold on
    h=plot(X(i,2), y(i), 'ko');
    set(h,'markerfacecolor', 'k');
    
    % prediction interval
    tmp = sort(ps, 'ascend');
    Q5 = tmp(floor(0.05*Nsamples));
    Q95 = tmp(floor(0.95*Nsamples));
    line([X(i,2) X(i,2)], [Q5 Q95]);
end

logregSATdemo

end

%%%%%%%%%
function bnew = proposal(w, Sigma)
    model.mu = zeros(1, length(w));
    model.Sigma = Sigma; 
    bnew = w + gaussSample(model); 
end    

function p = logpost(w, X, y, lambda)
    mu = 1 ./ (1 + exp(-X*w(:))); 
    p = sum( (y.*log(mu) + (1-y).*log(1-mu))) + lambda/2*sum(w.^2);
end









%{







stat = load('satData.txt'); % Johnson and Albert p77 table 3.1
% stat=[pass(0/1), 1, 1, sat_score, grade in prereq]
% where the grade in prereq is encoded as A=5,B=4,C=3,D=2,F=1
y = stat(:,1);
N = length(y);
X = stat(:,4);
X1 = [ones(N,1) X];

lambda = 1e-10;
model = logregBinaryFitL2IRLS(X,y, lambda);
beta = model.w;
C = model.C;

% MH
setSeed(1);
xinit = beta;
Nsamples = 10000;
lambda = 0;
sigmaMH = 1.5;
%targetArgs = {X,y,lambda};
%proposalArgs = {sigmaMH*C};
target = @(b) logpost(b, X1, y, lambda);
prop = @(b) proposal(b, sigmaMH*C);
[bs, acceptRatio] = metropolisHastings(target, prop, xinit, Nsamples);
%[bs, naccept] = metrop(@logpost, @proposal, xinit, Nsamples,  targetArgs, proposalArgs);

% trace plots
figure
for i=1:2
  subplot(2,2,i)
  plot(bs(:,i))
end

% samples
figure
subplot(2,2,1)
hist(bs(:,2))
title('b1 slope')
subplot(2,2,2)
plot(bs(:,1), bs(:,2), '.')
xlabel('b0'); ylabel('b1')
subplot(2,2,4)
hist(bs(:,1))
title('b0 intercept')

MLE =  xinit
postMean  = mean(bs,1)
postMedian = median(bs,1)
  
% visualize model fit for each training point
figure
[junk,perm] = sort(X,'ascend');
N = length(perm);
for ii=1:N
  i = perm(ii);
  ps = 1 ./ (1+exp(-X1(i,:)*bs')); % ps(s) = p(y=1|x(i,:), bs(s,:)) row vec

  plot(X(i,1), median(ps), 'o');
  hold on
  h=plot(X(i,1), y(i), 'ko');
  set(h,'markerfacecolor', 'k');
  
  % prediction interval
  tmp = sort(ps, 'ascend');
  Q5 = tmp(floor(0.05*Nsamples));
  Q95 = tmp(floor(0.95*Nsamples));
  line([X(i) X(i)], [Q5 Q95]);
end


%%%%%%%%%
function bnew = proposal(b, Sigma)
    model = struct('mu', zeros(1, length(b)), 'Sigma', Sigma);
    bnew = b + gaussSample(model);

function p = logpost(b, X, y, lambda)
logprior = 0;  % log(1)
offsetAdded = true;
fn = @(w)LogisticLossSimple(w, X, y);
p = -penalizedL2(b(:), fn, lambda) + logprior;
%p = -logregL2NLLgradHess(b(:), X, y, lambda, offsetAdded) + logprior;

%}
##### SOURCE END #####
--></body></html>