
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>svmLogregComparison</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-04-25"><meta name="m-file" content="svmLogregComparison"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Compare kernalized L1 logistic regession to an svm.</a></li><li><a href="#2">SVM</a></li><li><a href="#3">LR L1</a></li><li><a href="#4">LR L2 (no kernel)</a></li></ul></div><h2>Compare kernalized L1 logistic regession to an svm.<a name="1"></a></h2><pre class="codeinput"><span class="comment">%PMTKslow</span>
load <span class="string">crabs</span>
</pre><h2>SVM<a name="2"></a></h2><pre class="codeinput">gammas = logspace(-2, 2, 30);
Cvals  = logspace(-1, 3.5, 30);
SVMmodel = svmFit(Xtrain, ytrain, <span class="string">'kernel'</span>, <span class="string">'rbf'</span>, <span class="keyword">...</span>
    <span class="string">'C'</span>, Cvals, <span class="string">'kernelParam'</span>, gammas);
yhat = svmPredict(SVMmodel, Xtest);
svmNerrors = sum(yhat ~= ytest) <span class="comment">%0</span>
</pre><pre class="codeoutput">svmNerrors =
     0
</pre><h2>LR L1<a name="3"></a></h2><pre class="codeinput">lambdaL1 = 3e-7; SigmaL1 = 8.5; <span class="comment">% see logregKernelCrabsDemo for cross validation</span>
LRL1model = logregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambdaL1, <span class="keyword">...</span>
    <span class="string">'kernelFn'</span>, @kernelRbfSigma, <span class="string">'kernelParam'</span>, SigmaL1, <span class="string">'regType'</span>, <span class="string">'L1'</span>);
yhat = logregPredict(LRL1model, Xtest);
lrL1Nerrors = sum(yhat ~= ytest)
</pre><pre class="codeoutput">  iter fEvals         stepLen            f(w)  free
     1      2    1.00000e+000    5.54497e+001    80
     2      3    1.95454e-001    5.54417e+001   152
     3      4    3.72544e-001    5.54314e+001   128
     4      5    2.51539e+000    5.53934e+001   117
     5      6    7.57245e+000    5.52923e+001   129
     6      7    4.46627e+001    5.48059e+001   129
     7      8    7.76838e+001    5.41033e+001   129
     8      9    1.06579e+002    5.32777e+001   129
     9     10    3.70026e+001    5.31769e+001   129
    10     11    8.97250e-001    5.31751e+001   139
    11     12    2.23445e+000    5.31739e+001   151
    12     13    2.45020e+000    5.31719e+001   156
    13     14    5.26650e+000    5.31649e+001   158
    14     15    7.27402e+000    5.31478e+001   161
    15     16    1.02977e+001    5.31033e+001   161
    16     17    1.41115e+001    5.29569e+001   161
    17     19    5.17067e+000    5.29186e+001   161
    18     20    9.33225e+001    5.28192e+001   140
    19     21    2.71516e+001    5.26947e+001   140
    20     22    7.06234e+000    5.26098e+001   133
    21     23    6.25900e+001    5.24558e+001   131
    22     24    1.89834e+001    5.24454e+001   124
    23     25    1.85834e+000    5.24451e+001   138
    24     26    1.53773e+000    5.24447e+001   141
    25     27    4.25084e+000    5.24430e+001   146
    26     28    7.19933e+000    5.24387e+001   160
    27     29    2.82874e+001    5.24159e+001   151
    28     30    6.34694e+001    5.23640e+001   146
    29     31    1.54222e+002    5.22282e+001   143
    30     32    4.35899e+002    5.19090e+001   143
    31     33    3.67597e+002    5.13103e+001   139
    32     35    4.71813e+002    5.05623e+001   139
    33     36    2.00587e+003    4.82829e+001   158
    34     37    7.83113e+002    4.67124e+001   159
    35     38    6.44254e+003    3.84969e+001   159
    36     40    3.81519e+003    3.64798e+001   155
    37     41    2.82055e+003    3.42866e+001   153
    38     43    3.29002e+004    2.88631e+001   153
    39     44    2.17288e+003    2.36907e+001   153
    40     45    4.19100e+003    1.99600e+001   152
    41     46    5.97251e+003    1.91615e+001   151
    42     47    1.16957e+004    1.84394e+001   153
    43     48    7.32035e+003    1.81137e+001   159
    44     49    7.77501e+003    1.78721e+001   159
    45     50    5.23751e+003    1.76667e+001   157
    46     51    4.74616e+003    1.74125e+001   156
    47     52    5.03645e+003    1.69590e+001   156
    48     53    3.50749e+003    1.63157e+001   153
    49     54    4.78204e+003    1.58990e+001   154
    50     55    1.12695e+004    1.56721e+001   160
    51     56    6.86657e+003    1.56293e+001   159
    52     57    1.70030e+003    1.56260e+001   159
    53     58    1.49505e+002    1.56259e+001   161
    54     59    2.19467e+001    1.56259e+001   160
    55     60    1.94828e+002    1.56257e+001   160
    56     61    3.04492e+002    1.56252e+001   159
    57     62    3.90646e+002    1.56238e+001   157
    58     63    1.09145e+003    1.56169e+001   157
    59     64    9.24427e+002    1.56035e+001   158
    60     66    7.04442e+001    1.56012e+001   158
    61     68    2.57617e+001    1.56004e+001   158
    62     70    1.17314e+001    1.55998e+001   158
    63     72    1.62067e+001    1.55988e+001   158
    64     74    9.56155e+001    1.55922e+001   158
    65     76    9.97361e+001    1.55889e+001   157
    66     78    1.93442e+001    1.55878e+001   156
    67     79    3.73395e+002    1.55053e+001   156
    68     81    6.35354e+001    1.54997e+001   155
    69     83    2.33804e+001    1.54971e+001   155
    70     85    1.76505e+001    1.54943e+001   155
    71     87    1.71434e+001    1.54943e+001   155
    72     88    7.47741e+002    1.54472e+001   154
    73     90    9.90242e+001    1.54469e+001   153
    74     91    5.10498e+002    1.54136e+001   152
    75     92    3.09466e+003    1.53032e+001   152
    76     93    8.73080e+003    1.50964e+001   152
    77     95    1.70363e+001    1.50887e+001   162
    78     97    2.70484e+001    1.50851e+001   162
    79     99    2.63728e+001    1.50825e+001   162
    80    100    9.32371e+003    1.49535e+001   153
    81    101    4.26903e+003    1.48350e+001   152
    82    102    2.29161e+003    1.46874e+001   151
    83    104    9.99657e+001    1.46858e+001   160
    84    106    2.22813e+002    1.46313e+001   160
    85    108    2.02838e+001    1.46298e+001   153
    86    110    4.27866e+001    1.46273e+001   153
    87    111    5.54570e+003    1.45923e+001   160
    88    113    4.17795e+002    1.45869e+001   160
    89    114    7.96911e+003    1.45268e+001   160
    90    115    1.58609e+003    1.44324e+001   159
    91    117    2.01537e+002    1.44142e+001   158
    92    119    1.40570e+002    1.43977e+001   158
    93    121    1.70545e+002    1.43746e+001   158
    94    123    1.51385e+002    1.43517e+001   158
    95    125    1.42428e+002    1.43282e+001   157
    96    127    1.53451e+002    1.43046e+001   157
    97    129    3.20853e+003    1.39184e+001   161
    98    131    7.16924e+001    1.39101e+001   161
    99    133    1.11125e+002    1.38922e+001   161
   100    135    6.68093e+002    1.38191e+001   161
   101    137    1.95711e+002    1.37892e+001   161
   102    139    9.64191e+002    1.36776e+001   161
   103    141    1.44129e+002    1.36522e+001   161
   104    143    1.50531e+002    1.36265e+001   161
   105    145    1.50254e+002    1.36019e+001   161
   106    146    8.96703e+003    1.28551e+001   160
   107    147    4.32485e+003    1.19972e+001   159
   108    149    2.28058e+002    1.19703e+001   159
   109    151    2.06074e+002    1.19496e+001   158
   110    153    2.12516e+002    1.19261e+001   158
   111    155    2.17642e+002    1.19017e+001   158
   112    157    2.15026e+002    1.18774e+001   158
   113    159    2.10587e+002    1.18536e+001   158
   114    161    2.05523e+002    1.18303e+001   158
   115    163    2.00288e+002    1.18075e+001   158
   116    165    2.01482e+002    1.17846e+001   158
   117    167    1.92216e+002    1.17625e+001   158
   118    169    1.90551e+002    1.17407e+001   158
   119    171    1.76501e+002    1.17213e+001   158
   120    174    5.51429e+002    1.15383e+001   158
   121    176    1.77259e+002    1.15188e+001   162
   122    178    1.76071e+002    1.14996e+001   162
   123    180    1.73941e+002    1.14807e+001   162
   124    182    1.72106e+002    1.14621e+001   162
   125    184    1.70503e+002    1.14437e+001   162
   126    186    1.68936e+002    1.14269e+001   162
   127    188    1.06352e+002    1.14165e+001   161
   128    190    9.65045e+001    1.14074e+001   161
   129    192    1.61042e+002    1.13898e+001   161
   130    194    9.48159e+001    1.13817e+001   161
   131    196    7.35879e+001    1.13761e+001   160
   132    198    5.79038e+001    1.13718e+001   160
   133    200    5.77041e+001    1.13677e+001   160
   134    202    5.73927e+001    1.13636e+001   160
   135    204    5.70103e+001    1.13597e+001   160
   136    206    5.68373e+001    1.13560e+001   160
   137    208    3.37681e+001    1.13539e+001   159
   138    210    2.94495e+001    1.13520e+001   159
   139    212    2.96338e+001    1.13501e+001   159
   140    214    2.89405e+001    1.13480e+001   159
   141    216    2.99693e+001    1.13458e+001   159
   142    218    2.98833e+001    1.13434e+001   159
   143    220    3.06055e+001    1.13410e+001   159
   144    222    3.05662e+001    1.13389e+001   159
   145    223    7.98287e+001    1.13354e+001   158
   146    224    5.53617e+001    1.13310e+001   157
   147    225    7.53330e+002    1.12743e+001   157
   148    226    1.25699e+003    1.11529e+001   157
   149    227    6.10872e+003    1.10538e+001   157
   150    229    5.42142e+002    1.07320e+001   152
   151    231    1.61642e+002    1.06805e+001   152
   152    232    1.20025e+003    1.04822e+001   147
   153    233    2.47198e+003    1.02778e+001   156
   154    235    2.11235e+001    1.02763e+001   157
   155    237    1.93036e+001    1.02735e+001   157
   156    239    2.15222e+001    1.02700e+001   157
   157    241    2.70397e+001    1.02664e+001   157
   158    242    2.71035e+002    1.02525e+001   155
   159    243    2.34730e+002    1.02476e+001   159
   160    244    4.05337e+002    1.02410e+001   158
   161    245    1.49291e+002    1.02341e+001   154
   162    246    4.51973e+002    1.01897e+001   152
   163    247    3.06816e+002    1.01503e+001   149
   164    249    1.50200e+002    1.01349e+001   149
   165    251    1.03993e+002    1.01242e+001   149
lrL1Nerrors =
     8
</pre><h2>LR L2 (no kernel)<a name="4"></a></h2><p>Cross validates over both lambda and 'rbf' Gamma</p><pre class="codeinput">lambdas = logspace(-5,0,50);
[LRL2model, lambdaStar, LRmu, LRse] = logregFit(Xtrain, ytrain,<span class="keyword">...</span>
    <span class="string">'lambda'</span>, lambdas, <span class="string">'regType'</span>, <span class="string">'L2'</span>);
yhat = logregPredict(LRL2model, Xtest);
lrL2Nerrors = sum(yhat ~= ytest)
</pre><pre class="codeoutput">lrL2Nerrors =
     3
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
%% Compare kernalized L1 logistic regession to an svm. 
%PMTKslow
load crabs
%% SVM
gammas = logspace(-2, 2, 30);
Cvals  = logspace(-1, 3.5, 30);
SVMmodel = svmFit(Xtrain, ytrain, 'kernel', 'rbf', ...
    'C', Cvals, 'kernelParam', gammas);
yhat = svmPredict(SVMmodel, Xtest);
svmNerrors = sum(yhat ~= ytest) %0
%% LR L1
lambdaL1 = 3e-7; SigmaL1 = 8.5; % see logregKernelCrabsDemo for cross validation
LRL1model = logregFit(Xtrain, ytrain, 'lambda', lambdaL1, ...
    'kernelFn', @kernelRbfSigma, 'kernelParam', SigmaL1, 'regType', 'L1');
yhat = logregPredict(LRL1model, Xtest);
lrL1Nerrors = sum(yhat ~= ytest)
%% LR L2 (no kernel)
% Cross validates over both lambda and 'rbf' Gamma
lambdas = logspace(-5,0,50);
[LRL2model, lambdaStar, LRmu, LRse] = logregFit(Xtrain, ytrain,...
    'lambda', lambdas, 'regType', 'L2');
yhat = logregPredict(LRL2model, Xtest);
lrL2Nerrors = sum(yhat ~= ytest)

##### SOURCE END #####
--></body></html>